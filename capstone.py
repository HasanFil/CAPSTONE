# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EhBwICSKDzFZZ1t8doUVBS_L5yXCH48P

**IBM HR Analytics Employee Attrition & Performance**

Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.

Employee attrition is the gradual reduction in employee numbers. Employee attrition happens when the size of your workforce diminishes over time. This means that employees are leaving faster than they are hired.

Employee attrition happens when employees retire, resign, or simply aren’t replaced.

Although employee attrition can be company-wide, it may also be confined to specific parts of a business. This is often the case when employees are replaced by automation or the adoption of new technologies.

Employee attrition can happen for several reasons. These include unhappiness about employee benefits or the pay structure, a lack of employee development opportunities, and even poor conditions in the workplace.

In a world where the skill sets required are constantly changing, some positions also become obsolete over time. As employees leave and a new future of work emerges, not every role is filled in the same cookie-cutter way. 

With this, a new world of work means a new look at leadership. 

In some cases, this is driven by a desire to modernize. In others, it’s due to a lack of skilled younger talent in certain industries and geographies.

Employee attrition also refers to the downsizing of an organization’s workforce. This means that attrition can be voluntary or involuntary.

Employee attrition can be problematic as it often reduces talent within the company and the workforce in general. 

But employee attrition isn’t all bad. 

It can be positive because it allows the company to identify and address problematic issues for its employees. For example, a high attrition rate could be from employees leaving due to a poor workplace culture. Only by investigating the reasons for this employee attrition can management make changes to improve the organization’s work culture for other employees.

While companies will usually try to avoid employee attrition, it can sometimes help cut down costs associated with labor. It can also attract new employees with fresh talent to organizations.

Employee attrition, also known as employee turnover, refers to the rate at which employees leave an organization and are replaced by new hires. High rates of employee attrition can be costly for companies, as they may have to spend resources on training and onboarding new employees, as well as potentially experiencing a decrease in productivity due to the loss of experienced workers. To address employee attrition, organizations may focus on retaining their current employees through strategies such as offering competitive benefits, providing professional development opportunities, and creating a positive work culture. They may also try to reduce the likelihood of employees leaving by improving the hiring process to ensure that new hires are a good fit for the company. Ultimately, managing employee attrition is an important aspect of maintaining a healthy and successful organization.

**DATA ANALYSIS**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

employee_data = pd.read_csv('Employee-Attrition.csv')

"""**Exploratory Data Analysis**"""

employee_data.head()

employee_data.shape

"""Let's import the dataset and make of a copy of the source file for this analysis.

The dataset contains 1,470 rows and 35 columns.
"""

employee_data.columns

"""The dataset contains several numerical and categorical columns providing various information on employee's personal and employment details."""

employee_data.isnull().sum()

employee_data.info()

employee_data.describe()

"""count #How many records there are

average #Average value

std #Standard deviation value

min #minimum value in this series

25% #Average value of 25% of total records

50% #Average value of 50% of total records

75% #Average value of 75% of total records

max #Maximum value in this series
"""

employee_data.select_dtypes(include=['object']).dtypes

employee_data['Attrition'].value_counts()

employee_data['Attrition'] = employee_data['Attrition'].factorize(['No','Yes'])[0]
employee_data.head()

sns.countplot(x='Attrition',data=employee_data)

"""In our dataset, we showed the quitters and non-employeds in a column chart."""

plt.figure(figsize=(8,8))
pie = employee_data.groupby('Attrition')['Attrition'].count()
plt.pie(pie, explode=[0.1, 0.1], labels=['No', 'Yes'], autopct='%1.1f%%');

"""
As shown on the chart above, we see this is an imbalanced class problem. Indeed, the percentage of Current Employees in our dataset is 83.9% and the percentage of Ex-employees is: 16.1%"""

employee_data.select_dtypes(include=['int64']).dtypes

"""
We changed our data types."""

#Data Corr

"""Let's take a look at some of most significant correlations. It is worth remembering that correlation coefficients only measure linear correlations.

What is correlation?

Correlation is a statistical term describing the degree to which two variables move in coordination with one another. If the two variables move in the same direction, then those variables are said to have a positive correlation. If they move in opposite directions, then they have a negative correlation.
"""

employee_data.corr()

plt.figure(figsize=(35,20))
sns.heatmap(employee_data.corr(),annot=True,cmap='twilight_shifted')

"""Correlation is a statistical measure that reflects the strength and direction of a relationship between two variables. It is used to determine whether there is a relationship between two variables and, if so, how strong that relationship is. Correlation can be positive, negative, or zero. A positive correlation means that as one variable increases, the other variable also increases. A negative correlation means that as one variable increases, the other variable decreases.

There are many reasons why we might want to make a correlation. For example, we might want to understand the relationship between two variables in order to make predictions about future outcomes, to identify patterns or trends, to identify the causes of certain phenomena, or to understand the relationships between different variables in a system. Additionally, correlations can be used to identify the strength and direction of relationships between variables, which can be useful for making decisions or for designing interventions.

As we can see, the target column does not have a very strong correlation with any numeric column. However;

More senior employees have higher total years of work (very obvious) Higher performance grades lead to higher pay rise The more years an employee has, the higher their monthly income Over the years, many employees remain in their current role and under the same manager, which means they are not promoted, and this can be a major contributing factor to attrition. From this we can conclude that lack of promotion can be a very important factor for attrition.
"""

plt.figure(figsize=(8,5))
sns.kdeplot(x=employee_data['Age'],shade=True,label='Age')
plt.axvline(x=employee_data['Age'].mean(),color='k',linestyle ="--",label='Mean Age: 36.923')
plt.legend()
plt.title('Distribution of Age')
plt.show()

"""
The mean and distribution of age in the data set is shown in the graph."""

plt.figure(figsize=(8,5))
sns.kdeplot(x=employee_data['MonthlyIncome'],shade=True,label='Monthly Income')
plt.axvline(x=employee_data['MonthlyIncome'].mean(),color='k',linestyle ="--",label='Average: 6502.93')
plt.xlabel('Monthly Income')
plt.legend()
plt.title('Distribution of Monthly Income')
plt.show()

"""
The average and distribution of monthly income in the data set is shown in the graph."""

employee_data[['Age']].value_counts().sort_values(ascending=False).head(10)

employee_data[['Age']].value_counts().sort_values(ascending=False).tail()

sns.countplot(x='BusinessTravel',data=employee_data)

sns.countplot(x='Department',data=employee_data)

plt.figure(figsize=(8,8))
sns.countplot(x='EducationField',data=employee_data)

sns.countplot(x='Gender',data=employee_data)

sns.countplot(employee_data["YearsAtCompany"])

sns.boxplot(employee_data["YearsAtCompany"])

"""

Most employees stay with the company for 3-9 years, with a median of 5 years."""

sns.countplot(x='BusinessTravel', hue='Attrition', data=employee_data);

"""
Most employees who travel rarely leave the company. As far as we can tell, sending employees on business trips doesn't make much of a difference and doesn't have a significant impact on attrition."""

plt.figure(figsize=(8,6))
sns.countplot(x='Department', hue='Attrition', data=employee_data);

employee_data['Department'].value_counts()

"""Most attrition comes from the research and development department, with the sales department in the second place by a small margin. HUMAN resources have the least attrition. However, we should not forget that R&D has many more employees than sales and HR.

If we take into account the percentage of attrition per department, we see that the HR department has the most attrition.
"""

sns.countplot(x='Gender', hue='Attrition', data=employee_data);

"""
Clearly there are more men than women in the organization so the attrition is higher but only slightly. IW does not consider gender a major factor behind attrition."""

plt.figure(figsize=(8,6))
sns.countplot(x='JobRole', hue='Attrition', data=employee_data);
plt.xticks(rotation=90)

"""
Among job roles, most lab technicians left their jobs, with only research scientists, sales managers, and sales reps (% wise) left behind. We can look at the salaries of each job role and see if that is the reason."""

plt.figure(figsize=(10,6))
sns.barplot(x='JobRole', y='MonthlyIncome', hue='Attrition', data=employee_data)
plt.xticks(rotation=90)

"""
As suspected, the salaries of laboratory technicians, research scientists and sales representatives and managers are very low, and this may be a major factor behind attrition.

Also, as we've seen before, the HR department had the most attrition and we can see that their salaries are also very low, so it's something to think about once again."""

sns.countplot(x='EducationField', hue='Attrition', data=employee_data);
plt.xticks(rotation=45)

"""Employee ratings really matter here, as most of the attrition numbers are similar."""

sns.countplot(x='OverTime', hue='Attrition', data=employee_data);

"""
Overtime hours are also not a very important factor."""

sns.countplot(x='EnvironmentSatisfaction', data=employee_data);

"""

Most employees seem satisfied with the work environment."""

f, axes = plt.subplots(2, 2, figsize=(10, 8), 
                       sharex=False, sharey=False)

# Defining our colormap scheme
s = np.linspace(0, 3, 10)
cmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)

# Generate and plot
x = employee_data['Age'].values
y = employee_data['TotalWorkingYears'].values
sns.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])
axes[0,0].set( title = 'Age against Total working years')

cmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)

# Generate and plot
x = employee_data['YearsInCurrentRole'].values
y = employee_data['Age'].values
sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])
axes[0,1].set( title = 'Years in role against Age')

cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)

# Generate and plot
x = employee_data['DailyRate'].values
y = employee_data['JobSatisfaction'].values
sns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])
axes[1,0].set( title = 'Daily Rate against Job satisfaction')

cmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)
# Generate and plot
x = employee_data['YearsInCurrentRole'].values
y = employee_data['Age'].values
sns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[1,1])
axes[1,1].set( title = 'Years in role against Age')

cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)

f.tight_layout()

"""In this analysis, we found that 16% of employees left the company in the previous quarter and more than half left the Research and Development department. The highest circulation among women was in the Human Resources department. About 1 in 4 employees with the lowest work-life balance left the company, but the majority of employees who left rated their job satisfaction as good or excellent. Among former employees, the salary was found to be significantly lower and the median monthly income was approximately $2,000/month less than current employees.

**Outlier Analysis & Filtering**

To winsorize data means to set extreme outliers equal to a specified percentile of the data, in this case we are going to be conservative and we are going to stay with the 98% of every variable, filtering the top 1% and bottom 1%.
"""

def Winsorization_outliers(df):
    q1 = np.percentile(df , 1)
    q3 = np.percentile(df , 99)
    out=[]
    for i in df:
        if (i > q3 or i < q1) and i>0:
            out.append(i)
    print("Outliers:",out)
    return out;
def remove_outliers(df):
    print("Registers in the initial dataset:",df.shape[0])
    for col in df.columns[1:]:
        if df[col].dtype != 'object':
            print(col)
            data_filter = Winsorization_outliers(df[col])
            df = df[~df[col].isin(data_filter)]
            print("Registers without outliers in "+col+" :"+ str(df.shape[0]))
    return df;


employee_data_cleaned = remove_outliers(employee_data)

"""Drop columns"""

employee_data.drop('EmployeeCount',axis=1,inplace=True)
employee_data.drop('StandardHours',axis=1,inplace=True)

"""Show input X and output y"""

X=employee_data.drop('Attrition',axis=1)
y=employee_data.iloc[:,1]
key=X.keys()

"""Show X"""

X

"""Show y"""

y

"""Transform y"""

label=LabelEncoder()
y=label.fit_transform(y)
pd.DataFrame(y,columns=['Attrition'])

"""Transform X"""

object=['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']
for col in object:
    X[col]=label.fit_transform(X[col])
pd.DataFrame(X,columns=key)

"""**Feature Engineering**"""

from sklearn.preprocessing import LabelEncoder

label=LabelEncoder()
y=label.fit_transform(y)
pd.DataFrame(y,columns=['Attrition'])

object=['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']
for col in object:
    X[col]=label.fit_transform(X[col])
pd.DataFrame(X,columns=key)

"""What is Label Encoding?

Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.

What is a Dummy Variable?

A dummy variable is a numeric variable that encodes categorical information.

Dummy variables have two possible values: 0 or 1.

**MinMaxScaler for Data**
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
X = scaler.fit_transform(X)
pd.DataFrame(X,columns=key)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,classification_report
from sklearn.metrics import recall_score
from sklearn.neighbors import KNeighborsClassifier

"""**Split Data**"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.25,shuffle=True,random_state=33)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""**Applying RandomForestClassifier Model**"""

RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=100,max_depth=20,random_state=33) #criterion can be also : entropy 
RandomForestClassifierModel.fit(X[:1350], y[:1350])

"""**Calculating Details**"""

print('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))
print('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))
print('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)

"""**Applying KNeighborsClassifier Model**"""

KNNClassifierModel = KNeighborsClassifier(n_neighbors= 5,weights ='uniform', # it can be distance
                                          algorithm='auto') # it can be ball_tree, kd_tree,brute
KNNClassifierModel.fit(X[:1350], y[:1350])

"""**Calculating Details**"""

print('KNNClassifierModel Train Score is : ' , KNNClassifierModel.score(X_train, y_train))
print('KNNClassifierModel Test Score is : ' , KNNClassifierModel.score(X_test, y_test))

"""**Calculating Prediction**"""

y_pred = RandomForestClassifierModel.predict(X_test)
y_pred_prob = RandomForestClassifierModel.predict_proba(X_test)
print('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])
print('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])

"""**Calculating Confusion Matrix**"""

CM = confusion_matrix(y_test, y_pred)
CM

"""**Drawing Confusion Matrix**"""

sns.heatmap(CM, center = True)

"""**Calculating Confusion Matrix**"""

CM = confusion_matrix(y_train,RandomForestClassifierModel.predict(X_train))
CM

"""**Drawing Confusion Matrix**"""

sns.heatmap(CM, center = True)

"""**Calculating Accuracy Score : ((TP + TN) / float(TP + TN + FP + FN))**"""

AccScore = accuracy_score(y_test, y_pred, normalize=False)
print('Accuracy Score is : ', AccScore)

"""**Calculating F1 Score : 2 (precision recall) / (precision + recall)**"""

F1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples
print('F1 Score is : ', F1Score)

"""**Calculating Recall Score : (Sensitivity) (TP / float(TP + FN)) 1 / 1+2**"""

RecallScore = recall_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples
print('Recall Score is : ', RecallScore)

"""**Calculating Precision Score : (Specificity) #(TP / float(TP + FP))**"""

PrecisionScore = precision_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples
print('Precision Score is : ', PrecisionScore)

"""**Calculating classification Report**"""

ClassificationReport = classification_report(y_test,y_pred)
print('Classification Report is : ', ClassificationReport )

sub=[]
for i in y_pred:
    if i==0:
        sub.append('No')
    else:
        sub.append('yes')
submission=pd.DataFrame(sub,columns=['Attrition'])      

submission

"""**REFERENCES**

https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset

https://www.betterup.com/blog/employee-attrition#:~:text=Employee%20attrition%20is%20the%20gradual,or%20simply%20aren't%20replaced.

https://www.investopedia.com/terms/c/correlation.asp#:~:text=Correlation%20is%20a%20statistical%20term,they%20have%20a%20negative%20correlation

https://www.kaggle.com/code/robertobonilla/explained-100-advanced-classification-beginners

https://chat.openai.com/chat

https://scikit-learn.org/stable/index.html
"""